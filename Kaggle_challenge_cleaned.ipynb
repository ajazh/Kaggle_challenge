{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmOC4s4631vz"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers transformers\n",
    "!pip install -q lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513,
     "referenced_widgets": [
      "56fa84f56b0d4690a3c7d13c1ad546ba",
      "dde6155496f841c4be1cee1db9c6adfc",
      "42178e53c06045d280d7e5840e903711",
      "a770cca32a2942d2be79a8ffee27546f",
      "c66d401115bb45c49e51ec9d4e0c4703",
      "53674dd428b04d7ba368efe8395465c8",
      "c7e3624ed0b54afb8d3b9363993f5134",
      "131b0ea2f4c74e33a1fdc25b2fd240e7",
      "dba7fec7db5a4aedbc6de47fe012333d",
      "525f454ee6104a90b3da50e436aafced",
      "f89c44a9940b48089b932b6e568c99f4",
      "e8ede04b86814ee3afee9dda051eda02",
      "bdc8f7dcb649496d81ea9a6ecb010705",
      "9d8aa8e2f91841178536984e8e33f1d5",
      "0ea99ceca05c4195a31c0c89132f7cda",
      "fc981b87d9f24138bec4d25a1b5ce155",
      "1a8592fb9763438caeb2031e803cb0c9",
      "a0e79ab2a7664ac4abf7098a09dd5295",
      "06efdd3c07ca472991b568d6b0776e28",
      "a1f3adade31f479581da8bb442682bd5",
      "dddbe51baa084054bfa6c6200b11d186",
      "8d264084c82e4b01b48ea14108017d46",
      "4263500336df44e3b29dc1556c42c9d5",
      "1d4ebcf06c804751942a5c7722161157",
      "20562fcb2e58424eb9820695a3a29be6",
      "79f7d566ea124618beddfaef23c624db",
      "4f90fb52f5854ef0a9aa69e7e9be0cd7",
      "0d3021cfb518430d806c5fba36669332",
      "e7465116bcd44f62b8653f9af1a3b914",
      "8ebcba4120ce4ac6b6a8a08d45dca219",
      "9b37feda9ad44deabbad453883c41daa",
      "15b66d0cee45496a94083226e10b2f3a",
      "3ff086075e174fcfbf94eab19793796a",
      "3e3153ee63514ba681f2b776ce5fe1da",
      "e3ca52a17168412287833665b50a5c1f",
      "646b6aec84494929a4c74e7a296f8469",
      "a52bbef9c4fe42b9b56b28faac84b0d4",
      "df27833d80934c0391e2c7b3221bd1f3",
      "1b0d7be70d1d48e7be998185375c5765",
      "c4923765496e41719ec94d2f71e92173",
      "e41d0277e34b4d0bb998cdb23a17fc5f",
      "2b8ffb3a5c224ef3b7017e8fcc602faa",
      "4896bc7387fc474096c3f3e373c817c5",
      "deadc31b28da48e093c9a259953deff7",
      "6355ecc1f85744759bfaf7a7d8228d07",
      "22f6be0733bf490f9681b9bb75a7b17a",
      "c1521fb795214fed9b19351f36cd7f2f",
      "bf55b03abf3f449f8029ac4b68d62013",
      "fd5dc236c00842de9879b079cd67061a",
      "06b053939cf64b7784b17cf1bca086b8",
      "0f3decba91f74a5e85250ece033b29e5",
      "1f8397c9bb7243e5bbdc93172bcaed06",
      "b4d088ab0ab2417bb0309785a317c0f0",
      "a7f9c7c1685148f8b2b091eda3c330c9",
      "f8dec30903db4f529934abb322ae924f",
      "9c786a0478564ac4960034a2201d39f2",
      "899bdaa68da44ceba838182464636857",
      "122daf6d2e8c4eec82d4893d708f5190",
      "e9a34c7a315f4a969b2c60c272a81731",
      "35f24d3f71b94b36bb53784133c01d03",
      "7722b9625ef5402ea5c65d44ab3c7001",
      "a767d58b54c44071950b8277fb281939",
      "d68665974a44400d9fd4e5cbef461130",
      "85b13b6cb73548f28df90eb4003c09a9",
      "152f6ddffd4f4548b7cca2910c662c1f",
      "7bf8dfa722be4c64b14276a55ad8222f",
      "d6c2b5dc496543e9bee02fec41950b83",
      "80e3210d353b47ebbbe1944c32a63560",
      "379d772ef1bf49989fb805e44590a3c9",
      "f2476d345d364f4da67b1c63b5d01ac6",
      "4508b02607484b0bb60d6c52d4f10978",
      "e1b2ac4cd59c4b20ac557c59fd49693f",
      "8da91a82d47c47ee82af073b2e9ac33b",
      "062a5f2cbef7427db7e30bb1fc0f0eef",
      "42408babb82d46d5a5aa5befe11de915",
      "8527bc866a6a47ac8c90ecce001b8224",
      "453951f4cd1643dd8c0848961cacbe0f",
      "0a8dfd3df8ba4a5a94eb70c0ae737a12",
      "fd2f840938484b60b4645ad9df5cd5a4",
      "90a3a09638ee49ac926df0e1c249eb91",
      "253271ebdf38414d989e7b7af9a604a0",
      "b0768a4f093a474bb9624d4070f27e08",
      "f49bcc20a9e2470d9ef2499837f5a234",
      "941d8447200946318e707a5871702dc1",
      "bde5515e6b2f47bf9bbf20b5bc0262fc",
      "a0f9e244190a49758f51c644903b3574",
      "9aab4926776b4706b8be30765a850e73",
      "e93a2e6adb1140e1afdd9073607febf6",
      "26b33ca2e83a424dad7d80b2ff5b7dcb",
      "948f9860542743bd950eec8cb98bc4fe",
      "0f2601e745724f69bc6e2545cb0ec75c",
      "c56daf2da2964a658e7eb1906a250edd",
      "69396a60f0ad4ce594e72700a0df95a5",
      "8daf458d408c4d3d835ebe73f5c7d5d8",
      "4054168574c64037863b00009645dc2e",
      "9256cecd20ac4081b517fad5580e0dd9",
      "dae0873a6d694f7c8b3cba96e1f694f4",
      "0551bf6d045d47a5aa7bba5ceb816b7e",
      "d5584ec651374fa4bd24cb719f022c98",
      "e804c13744434cf7a41e1a07226e32ca",
      "b3171e5646644531a288f6012aafe7cc",
      "eaaf699dd2c74d01beffcf3c0995fdb0",
      "52f1051b0f1c4c5cabbaa8356c275743",
      "edca4beb4b3b42b8aaf7d499fde92b3f",
      "e90325459b7a47cead978412cbb1fedb",
      "90caeea70cb44b3c8be01f362e4bd236",
      "840a88da328c46918acaa18c7d61b50a",
      "a431011617154d27b6c20b247059aaaf",
      "3f119be553f7429186f7682091596078",
      "26c1358c3a024795851ad31b484e7013"
     ]
    },
    "id": "7WFpQrt66JR7",
    "outputId": "c729ccd5-d773-4520-c030-a9e505e8fa05"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"Transformers OK:\", transformers.__version__)\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "print(\"Model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "S82jHKUU6nui",
    "outputId": "8bf8832b-f909-4103-b3d1-d6ba84ad2793"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON files\n",
    "with open(\"train_data.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    train_raw = json.load(f)\n",
    "\n",
    "with open(\"test_data.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    test_raw = json.load(f)\n",
    "\n",
    "with open(\"metric_names.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    metric_map = json.load(f)\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_raw)\n",
    "test_df = pd.DataFrame(test_raw)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Show 3 samples\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "YUhPFX4F7CYp",
    "outputId": "b7fccedc-3da4-4e19-d5b4-587dc3a5c101"
   },
   "outputs": [],
   "source": [
    "def combine_all(row):\n",
    "    sp = str(row.get(\"system_prompt\", \"\")) if row.get(\"system_prompt\") else \"\"\n",
    "    up = str(row.get(\"user_prompt\", \"\"))\n",
    "    rp = str(row.get(\"response\", \"\"))\n",
    "\n",
    "    return sp + \" [SYS] \" + up + \" [USR] \" + rp + \" [RES]\"\n",
    "\n",
    "train_df[\"combined_text\"] = train_df.apply(combine_all, axis=1)\n",
    "test_df[\"combined_text\"]  = test_df.apply(combine_all, axis=1)\n",
    "\n",
    "train_df[[\"combined_text\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_378UpDK8trs",
    "outputId": "00c87d7a-37f2-41ad-8966-009b8b1f46b7"
   },
   "outputs": [],
   "source": [
    "train_df[\"metric_text\"] = train_df[\"metric_name\"].astype(str)\n",
    "test_df[\"metric_text\"]  = test_df[\"metric_name\"].astype(str)\n",
    "\n",
    "train_df[[\"metric_name\", \"metric_text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220,
     "referenced_widgets": [
      "3b2e1abe6a604f6998b86684df8ca478",
      "b4c9f1b774ab45beb8de2d0b64d6058b",
      "90035162349c45f5a1b2231552075d79",
      "bb15d8acc85343ffb4224e0de263f58a",
      "1a15635c18f147d4a03f7fabfc5eb619",
      "c7665f8e582545779528c8ec8cf69416",
      "c2d26fa589274265b06a3c91dc55c629",
      "a0d72aee21674ee6a0b700f5577f3985",
      "312062c9ab8d439ab68ae48d40ee8e0f",
      "e64a0a7bdfde45488bf70177ac34ff37",
      "c8c58c71aced454a9afe6e5f831e9c32",
      "66294dcf414445ef941f79787a63ad2a",
      "6799a1a08d954fc6a1b1bf5de5d8d76f",
      "a4866559bd404107b2ae5dc0f832f909",
      "330da46d78a34fc38bd70c35ae769ddf",
      "9cbba164b17d448e94d5d180d7391b8f",
      "154ceb51a590411bbb70c4009da27caa",
      "8d0c8c52cb414a39a03f0eac12fcc560",
      "13511925dd054b268d132eaddb596d66",
      "b4dec6e5efcb498d9468dd4bdd1a215f",
      "1b15f12dbff7408aa0e7574ec26020c8",
      "a90699f4f7ae423c9f07e7694e6b728b"
     ]
    },
    "id": "sonpQN5e7OzF",
    "outputId": "fb1307e2-efdf-4dc1-9245-b0d17e96224a"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# Prepare metric_text values\n",
    "unique_metric_texts = train_df[\"metric_text\"].unique().tolist()\n",
    "\n",
    "# Dictionary to store metric embeddings\n",
    "metric_emb_dict = {}\n",
    "batch_size = 64\n",
    "\n",
    "# Embed metric names\n",
    "print(\"Embedding metric_text values...\")\n",
    "for i in tqdm(range(0, len(unique_metric_texts), batch_size)):\n",
    "    batch = unique_metric_texts[i:i+batch_size]\n",
    "    batch_emb = model.encode(batch, batch_size=batch_size, convert_to_numpy=True)\n",
    "    for txt, emb in zip(batch, batch_emb):\n",
    "        metric_emb_dict[txt] = emb\n",
    "\n",
    "# Build aligned metric embedding arrays\n",
    "train_metric_embs = np.vstack([metric_emb_dict[t] for t in train_df[\"metric_text\"]])\n",
    "test_metric_embs  = np.vstack([metric_emb_dict[t] for t in test_df[\"metric_text\"]])\n",
    "\n",
    "# Embed combined text (system + user + response)\n",
    "print(\"Embedding combined prompt/response texts...\")\n",
    "train_text_embs = model.encode(\n",
    "    train_df[\"combined_text\"].tolist(),\n",
    "    batch_size=batch_size,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "test_text_embs = model.encode(\n",
    "    test_df[\"combined_text\"].tolist(),\n",
    "    batch_size=batch_size,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the embeddings\n",
    "np.save(\"train_metric_embs.npy\", train_metric_embs)\n",
    "np.save(\"test_metric_embs.npy\", test_metric_embs)\n",
    "np.save(\"train_text_embs.npy\", train_text_embs)\n",
    "np.save(\"test_text_embs.npy\", test_text_embs)\n",
    "\n",
    "print(\"Embedding shapes:\")\n",
    "print(\"train_metric_embs:\", train_metric_embs.shape)\n",
    "print(\"train_text_embs :\",  train_text_embs.shape)\n",
    "print(\"test_metric_embs :\",  test_metric_embs.shape)\n",
    "print(\"test_text_embs :\",   test_text_embs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpUdJ4oH7sAG",
    "outputId": "b4e0cd32-9a30-40ea-8c09-976d0b988178"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Load embeddings again (to combine)\n",
    "train_metric = np.load(\"train_metric_embs.npy\")\n",
    "train_text   = np.load(\"train_text_embs.npy\")\n",
    "y_real       = train_df[\"score\"].values.astype(np.float32)\n",
    "\n",
    "N = len(train_metric)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 1) Shuffle-based negatives\n",
    "# --------------------\n",
    "perm = rng.permutation(N)\n",
    "neg_metric_1 = train_metric\n",
    "neg_text_1   = train_text[perm]   # mismatched\n",
    "neg_y_1      = rng.integers(0, 3, size=N)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 2) Noise-corrupted negatives\n",
    "# --------------------\n",
    "noise = rng.normal(scale=0.6, size=train_text.shape)\n",
    "neg_metric_2 = train_metric\n",
    "neg_text_2   = train_text + noise\n",
    "neg_y_2      = rng.integers(0, 3, size=N)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# 3) Metric swap negatives\n",
    "# --------------------\n",
    "perm2 = rng.permutation(N)\n",
    "neg_metric_3 = train_metric[perm2]  # mismatched metric\n",
    "neg_text_3   = train_text\n",
    "neg_y_3      = rng.integers(0, 3, size=N)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Combine everything\n",
    "# --------------------\n",
    "m_all = np.vstack([train_metric, neg_metric_1, neg_metric_2, neg_metric_3])\n",
    "t_all = np.vstack([train_text,   neg_text_1,   neg_text_2,   neg_text_3])\n",
    "y_all = np.concatenate([y_real,  neg_y_1,      neg_y_2,      neg_y_3]).astype(np.float32)\n",
    "\n",
    "print(\"Combined shapes:\")\n",
    "print(\"m_all:\", m_all.shape)\n",
    "print(\"t_all:\", t_all.shape)\n",
    "print(\"y_all:\", y_all.shape)\n",
    "\n",
    "np.save(\"m_all.npy\", m_all)\n",
    "np.save(\"t_all.npy\", t_all)\n",
    "np.save(\"y_all.npy\", y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUdr8lXA71uG",
    "outputId": "72098072-0de4-4347-f085-af61c15e3b5d"
   },
   "outputs": [],
   "source": [
    "# Step 4 — Build features (concat, absdiff, prod, cosine) and save\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load combined embeddings + labels (you already saved them)\n",
    "m_all = np.load(\"m_all.npy\")      # shape (20000, 1024)\n",
    "t_all = np.load(\"t_all.npy\")      # shape (20000, 1024)\n",
    "y_all = np.load(\"y_all.npy\")      # shape (20000,)\n",
    "\n",
    "# cosine similarity\n",
    "dot = np.sum(m_all * t_all, axis=1)\n",
    "norms = (np.linalg.norm(m_all, axis=1) * np.linalg.norm(t_all, axis=1)) + 1e-9\n",
    "cos = (dot / norms).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# elementwise features\n",
    "absdiff = np.abs(m_all - t_all).astype(np.float32)\n",
    "prod = (m_all * t_all).astype(np.float32)\n",
    "\n",
    "# concat metric and text\n",
    "concat = np.hstack([m_all.astype(np.float32), t_all.astype(np.float32)])  # (N, 2048)\n",
    "\n",
    "# final X\n",
    "X = np.hstack([concat, absdiff, prod, cos]).astype(np.float32)  # (N, 4097)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y_all.shape)\n",
    "print(\"sample cosines min/max:\", float(cos.min()), float(cos.max()))\n",
    "\n",
    "# (Optional) Save a scaler fitted on the training features (you can also fit inside the training loop)\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X)\n",
    "# import joblib\n",
    "# joblib.dump(scaler, \"feature_scaler.joblib\")\n",
    "\n",
    "# Save features and labels\n",
    "np.save(\"X_all.npy\", X)\n",
    "np.save(\"y_all.npy\", y_all)\n",
    "\n",
    "print(\"Saved X_all.npy and y_all.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFCRyVey3QM_",
    "outputId": "5dd591d2-004d-4dd6-bd2b-3f33c581ddf1"
   },
   "outputs": [],
   "source": [
    "# NLL-MLP on existing embeddings (classes 0..10) — reproducibility test\n",
    "# Paste & run in your notebook. Assumes X_all.npy, y_all.npy exist and test_metric_embs/test_text_embs exist.\n",
    "\n",
    "import os, numpy as np, pandas as pd, random, time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Load features / build test features if needed\n",
    "# -------------------------\n",
    "X = np.load(\"X_all.npy\").astype(np.float32)     # (N, 4097)\n",
    "y = np.load(\"y_all.npy\").astype(np.float32)     # (N, )\n",
    "\n",
    "# If X_test.npy exists use it, otherwise build from test embeddings\n",
    "if os.path.exists(\"X_test.npy\"):\n",
    "    X_test = np.load(\"X_test.npy\").astype(np.float32)\n",
    "else:\n",
    "    print(\"X_test.npy not found — building from test_metric_embs.npy + test_text_embs.npy\")\n",
    "    tm = np.load(\"test_metric_embs.npy\")\n",
    "    tt = np.load(\"test_text_embs.npy\")\n",
    "    dot = np.sum(tm * tt, axis=1)\n",
    "    norms = (np.linalg.norm(tm, axis=1) * np.linalg.norm(tt, axis=1)) + 1e-9\n",
    "    cos = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "    absdiff = np.abs(tm - tt).astype(np.float32)\n",
    "    prod = (tm * tt).astype(np.float32)\n",
    "    concat = np.hstack([tm.astype(np.float32), tt.astype(np.float32)])\n",
    "    X_test = np.hstack([concat, absdiff, prod, cos]).astype(np.float32)\n",
    "    np.save(\"X_test.npy\", X_test)\n",
    "    print(\"Saved X_test.npy\")\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \"X_test:\", X_test.shape, \"y:\", y.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Convert labels to integer classes 0..10\n",
    "# -------------------------\n",
    "y_int = np.rint(y).astype(int)  # round if necessary\n",
    "y_int = np.clip(y_int, 0, 10)\n",
    "assert y_int.min() >= 0 and y_int.max() <= 10\n",
    "\n",
    "# -------------------------\n",
    "# Dataset + model\n",
    "# -------------------------\n",
    "class EmbClsDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class NLL_MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1=1024, hidden2=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 11)   # 11 classes: 0..10 logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # logits\n",
    "\n",
    "# -------------------------\n",
    "# Training params\n",
    "# -------------------------\n",
    "NFOLDS = 5\n",
    "EPOCHS = 12\n",
    "BATCH = 256\n",
    "LR = 1e-3\n",
    "WD = 1e-5\n",
    "PATIENCE = 3   # early stop patience on val rmse\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_probs = np.zeros((len(X), 11), dtype=np.float32)\n",
    "test_probs_folds = np.zeros((NFOLDS, X_test.shape[0], 11), dtype=np.float32)\n",
    "\n",
    "# -------------------------\n",
    "# Train folds\n",
    "# -------------------------\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y_int[tr_idx], y_int[val_idx]\n",
    "\n",
    "    train_ds = EmbClsDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbClsDataset(X_val, y_val)\n",
    "    test_ds  = EmbClsDataset(X_test)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = NLL_MLP(in_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_rmse = 1e9\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                val_probs.append(probs)\n",
    "        val_probs = np.concatenate(val_probs, axis=0)\n",
    "        val_exp = (val_probs * np.arange(11)[None, :]).sum(axis=1)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y[val_idx], val_exp))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {ep}/{EPOCHS} | train_loss={running_loss/len(train_ds):.4f} | val_RMSE={val_rmse:.4f} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # early stopping\n",
    "        if val_rmse < best_rmse - 1e-5:\n",
    "            best_rmse = val_rmse\n",
    "            best_state = model.state_dict()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # OOF probs\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            val_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    val_probs = np.concatenate(val_probs, axis=0)\n",
    "    oof_probs[val_idx] = val_probs\n",
    "\n",
    "    # test probs for this fold\n",
    "    fold_test_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            fold_test_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    fold_test_probs = np.concatenate(fold_test_probs, axis=0)\n",
    "    test_probs_folds[fold] = fold_test_probs\n",
    "\n",
    "    print(f\"Fold {fold} best val RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate OOF, Calibration, Final test preds\n",
    "# -------------------------\n",
    "oof_exp = (oof_probs * np.arange(11)[None, :]).sum(axis=1)\n",
    "oof_rmse = np.sqrt(mean_squared_error(y, oof_exp))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse)\n",
    "\n",
    "# calibrate with linear regression\n",
    "cal = LinearRegression().fit(oof_exp.reshape(-1,1), y)\n",
    "oof_cal = cal.predict(oof_exp.reshape(-1,1))\n",
    "print(\"OOF RMSE (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n",
    "print(\"Calibration params: a=\", cal.coef_[0], \"b=\", cal.intercept_)\n",
    "\n",
    "# final test expected score (mean over fold probs -> expected -> calibrate)\n",
    "test_probs_mean = test_probs_folds.mean(axis=0)\n",
    "test_exp = (test_probs_mean * np.arange(11)[None,:]).sum(axis=1)\n",
    "test_exp_cal = cal.predict(test_exp.reshape(-1,1))\n",
    "test_exp_cal = np.clip(test_exp_cal, 0, 10)\n",
    "\n",
    "# Save artifacts\n",
    "np.save(\"oof_probs_nll.npy\", oof_probs)\n",
    "np.save(\"test_probs_nll.npy\", test_probs_mean)\n",
    "np.save(\"oof_nll.npy\", oof_exp)\n",
    "np.save(\"test_nll.npy\", test_exp)\n",
    "\n",
    "# Submission\n",
    "# make sure test_df exists (we used earlier)\n",
    "if 'test_df' not in globals():\n",
    "    import json\n",
    "    with open(\"test_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "        tdata = json.load(f)\n",
    "    test_df = pd.DataFrame(tdata)\n",
    "\n",
    "test_df[\"ID\"] = np.arange(1, len(test_exp_cal)+1)\n",
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": test_exp_cal})\n",
    "sub.to_csv(\"submission_nll_mlp.csv\", index=False)\n",
    "print(\"\\nSaved submission_nll_mlp.csv\")\n",
    "print(\"OOF RMSE final (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMHB3reK5ura"
   },
   "source": [
    "NEGATIVE LOG LIKELIHOOD UPAR WLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571,
     "referenced_widgets": [
      "bdbdfe5eb51f4a1f91b057756d71ae5f",
      "c1496a961daa4d2ea4f0ac95885e994f",
      "733d9ea6537b4555ab507e4e5f892327",
      "b48fb353e4df425fa043287ea85da4c1",
      "7fd78ceb933447f38f5d04609ec58c8a",
      "3b564e81fa6e426996b5654516a5e20e",
      "8536105667bc4aac825f0a2ba7718afd",
      "603ab92026014c5cbc25d55a70e528b0",
      "2f341d12aaeb42ae8e121994b9cb0961",
      "05840670c24f4eb88097b523082de16d",
      "3405b5eb13984c45905e9ded07dca484",
      "10cc9f8bf85742b78ab75ecfc1591c7a",
      "4ef6ab3158ba4fd09f063fe68972fc28",
      "d26bbb2e60904c208fe918e7ba5158b7",
      "0203561284c541c99641a2d85b50b60c",
      "3c860a96948a4c62bd7a338922482e46",
      "f0356e2db3a94df0a9c57acfd7be3051",
      "1e2507898ce442a8aba91e99b95808ac",
      "b49f33700b6e4b6dbb7659795ea19d47",
      "afd0e58a34254a54ac802b5b49c04f1d",
      "211948b03c324ff6a476c66ec474d2b4",
      "f501844651ad4dba848ecb0bd8534cfa",
      "b5850fe96e4e4a5aa3de28e42e8093bb",
      "a1bb9ea4d3ec40219a7ecf8081804770",
      "0874aa082a48462084d17d8845687730",
      "ac314dfce23143e889dbf86dc484469a",
      "f23d317f8c4741d7943e3b2a1e538276",
      "dcc6eb23bc7546598c5364d67e50e5ad",
      "619e0a7e6c084af39aece7656266839d",
      "ad7089b641c44454b40360b50bbc4c6f",
      "8ee5a41f46cf48678badc1e8314404c9",
      "3d56089eb68147d68b403af286f8cdee",
      "b2e5be0fb0e640fc9219b483d6482ffd",
      "ad545323d02f48b38d71bb9ff6c23b59",
      "533ab21ed6e44978babf6bc0f955770c",
      "57e4b031488d461e874ae9f059a0b1ee",
      "9ce1cbafa7614480af8bad2c4c497bfb",
      "57e684b740bd44b8a0d219d615b8ce62",
      "3ea89a525787498aae17cdf46caa72af",
      "3880d649c22f44d7a2a1a4f49b360d9a",
      "ed2144b7890044fba06f6e551bb9a5e6",
      "6fa9c3c4f46b405ca2e58bbb7ed57a11",
      "4bf84fe5c2cc41a5aaae3560472d5dd1",
      "e0ca245acfc246519931a52da47fe0d4",
      "588488aefd60454884c8c6de97f09bdb",
      "0244cb5cd4894d8fb4242ca5fbfb301d",
      "9e4c3bcb364445e380243049dac24423",
      "7a9c2b14f9294dac95aa475850b70b86",
      "ed923daf5fba4a85827ed891fb36ddc8",
      "15b6e927084445ff851732d94fabe495",
      "81464cb7a5ce45259df89654a46ebc43",
      "15f0e9a4d70347fba4be67005fd8386e",
      "0acfd27697ec4009bc8b6deaa60287f2",
      "3ec5c2456b6344a796892c1e4698b7a4",
      "98a8a47aa1d74bcdbb107cf72becf43f",
      "63666e9a2e1f4e9cb5dadb44fa4a4e35",
      "c6c900644ff24c79825f534edba9ec80",
      "fbb31a2d51de42de87b499238de8d31c",
      "5a50ea97f7bd4b59836ed67719e0798d",
      "dadf2b90d8e945fcb1193230b7d82774",
      "802e003e51b44c9b8020f9cdb9b54115",
      "d18145e062704c32a3d017be73a78929",
      "addf21e3f28b4d51ab28ac09f5015fb4",
      "b7a8689c10f449ddb614c710584a842d",
      "00f9209371054236a8fdee2b436e00a8",
      "7edd571d5a7d4d58924c6d38c3879705",
      "478b89b61cde42cbadf1df5c7c3b8cf9",
      "11bb7e1b410b4099bcc60c7cbb72fb56",
      "f07522961f2843edb3a0d244e64e16b4",
      "c2321607d9b2462896d30343c81ee633",
      "e395923c903241a985b52bcc7e131108",
      "0bdbdf33c72f418b9359bc02afb40d1f",
      "4f99cb4e5c4d4288be1af852f3f1196f",
      "f6c977eda766408a83c3d0e4bfb3bb0c",
      "4b25ef557ec249d488aa783b225d135d",
      "8b06dca710364cbb80e43117807ee218",
      "19b5538f55f04d37a357200a70072c8a",
      "169bd541553c42509aeae7b7e91fedb7",
      "48d6403dc4ce4c9aa64e4c7bd7aab40b",
      "c0fb352cb9d14a5aa77f8dc41d1d457e",
      "ca665c1655a946b39e50e9f66b8dbcb8",
      "cf0bb72f09614300b05d8e415f40a165",
      "93cb6c9995c043dfacd53cda5bcc03ff",
      "20198b460d0448cdb37678b45b5f9106",
      "93adfe9ba58b42bf8e74a481f6dd5a03",
      "a5477fc276684c16951d7f5c3c38918d",
      "16ae37ca646b4c14b32d7c42dec2e4a2",
      "3667b5fdbb994b9dab88d0512d77632d"
     ]
    },
    "id": "KubyIvv_9Auv",
    "outputId": "e36fa9e9-748b-483d-f7bb-386cf9cc8929"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"Alibaba-NLP/gte-large-en-v1.5\")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFu9eF2j920t",
    "outputId": "52a87e73-8875-4410-c714-5e1111b2b154"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Loaded successfully! Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elZzGwnM9-Up",
    "outputId": "7dc9974f-3432-4bcf-8d26-a46f78fafcd5"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# FULL CELL: compute GTE embeddings, build features, train NLL-MLP (KFold)\n",
    "# Paste & run in the notebook where you loaded GTE model/tokenizer and confirmed CUDA.\n",
    "# ------------------------------\n",
    "import os, time, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -----------------------\n",
    "# Config (tweak if needed)\n",
    "# -----------------------\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"   # you're already loaded, but kept for clarity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "ENC_BATCH = 32          # batch for encoding (reduce to 8/16 if OOM)\n",
    "MAX_LEN = 512\n",
    "\n",
    "# -----------------------\n",
    "# Ensure tokenizer & model exist in this session\n",
    "# (If not loaded already, load them; otherwise reuse)\n",
    "# -----------------------\n",
    "try:\n",
    "    tokenizer  # if defined earlier\n",
    "    model\n",
    "except NameError:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -----------------------\n",
    "# Load augmented train (or fallback to original train)\n",
    "# -----------------------\n",
    "if os.path.exists(\"augmented_train.csv\"):\n",
    "    df = pd.read_csv(\"augmented_train.csv\")\n",
    "    # expected columns: metric_text, combined_text, label (or score)\n",
    "    if \"label\" not in df.columns and \"score\" in df.columns:\n",
    "        df = df.rename(columns={\"score\": \"label\"})\n",
    "else:\n",
    "    # fallback: build from train_data.json\n",
    "    import json\n",
    "    with open(\"train_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "        r = json.load(f)\n",
    "    df = pd.DataFrame(r)\n",
    "    def combine_all(row):\n",
    "        sp = str(row.get(\"system_prompt\",\"\")) if row.get(\"system_prompt\") else \"\"\n",
    "        up = str(row.get(\"user_prompt\",\"\")) if row.get(\"user_prompt\") else \"\"\n",
    "        rp = str(row.get(\"response\",\"\")) if row.get(\"response\") else \"\"\n",
    "        return sp + \" [SYS] \" + up + \" [USR] \" + rp + \" [RES]\"\n",
    "    df[\"combined_text\"] = df.apply(combine_all, axis=1)\n",
    "    df[\"metric_text\"] = df[\"metric_name\"].astype(str)\n",
    "    df = df.rename(columns={\"score\":\"label\"})\n",
    "\n",
    "print(\"Loaded train rows:\", len(df))\n",
    "# keep only first N if debugging\n",
    "# df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# load test_data for final inference\n",
    "import json\n",
    "with open(\"test_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "    test_raw = json.load(f)\n",
    "test_df = pd.DataFrame(test_raw)\n",
    "def combine_all(row):\n",
    "    sp = str(row.get(\"system_prompt\",\"\")) if row.get(\"system_prompt\") else \"\"\n",
    "    up = str(row.get(\"user_prompt\",\"\")) if row.get(\"user_prompt\") else \"\"\n",
    "    rp = str(row.get(\"response\",\"\")) if row.get(\"response\") else \"\"\n",
    "    return sp + \" [SYS] \" + up + \" [USR] \" + rp + \" [RES]\"\n",
    "test_df[\"combined_text\"] = test_df.apply(combine_all, axis=1)\n",
    "test_df[\"metric_text\"] = test_df[\"metric_name\"].astype(str)\n",
    "print(\"Loaded test rows:\", len(test_df))\n",
    "\n",
    "# -----------------------\n",
    "# Helper: mean-pool to get sentence embedding\n",
    "# -----------------------\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    # last_hidden_state: (B, L, D); attention_mask: (B, L)\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    masked = last_hidden_state * mask\n",
    "    summed = masked.sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    return mean_pooled\n",
    "\n",
    "# -----------------------\n",
    "# Encode helper (batched)\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, batch_size=32, prefix_tokenize=None):\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", ncols=80):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # optionally prepend metric or other prefix in the string if needed\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attn = enc[\"attention_mask\"].to(device)\n",
    "        out = model(input_ids=input_ids, attention_mask=attn, output_hidden_states=True, return_dict=True)\n",
    "        last = out.last_hidden_state  # (B, L, D)\n",
    "        pooled = mean_pool(last, attn)  # (B, D)\n",
    "        embs.append(pooled.cpu().numpy())\n",
    "    embs = np.vstack(embs)\n",
    "    return embs\n",
    "\n",
    "# -----------------------\n",
    "# Build text lists to encode\n",
    "# For metric_text we can encode the metric string alone\n",
    "# For combined_text we encode as-is (system + user + response)\n",
    "# -----------------------\n",
    "train_metric_texts = df[\"metric_text\"].astype(str).tolist()\n",
    "train_combined_texts = df[\"combined_text\"].astype(str).tolist()\n",
    "test_metric_texts = test_df[\"metric_text\"].astype(str).tolist()\n",
    "test_combined_texts = test_df[\"combined_text\"].astype(str).tolist()\n",
    "\n",
    "# Encode (may take a couple minutes)\n",
    "print(\"Encoding train metric_texts...\")\n",
    "train_metric_embs = encode_texts(train_metric_texts, batch_size=ENC_BATCH)\n",
    "print(\"Encoding train combined_texts...\")\n",
    "train_text_embs = encode_texts(train_combined_texts, batch_size=ENC_BATCH)\n",
    "print(\"Encoding test metric_texts...\")\n",
    "test_metric_embs = encode_texts(test_metric_texts, batch_size=ENC_BATCH)\n",
    "print(\"Encoding test combined_texts...\")\n",
    "test_text_embs = encode_texts(test_combined_texts, batch_size=ENC_BATCH)\n",
    "\n",
    "print(\"Shapes:\", train_metric_embs.shape, train_text_embs.shape, test_metric_embs.shape, test_text_embs.shape)\n",
    "\n",
    "# save embeddings (so we can reuse)\n",
    "np.save(\"train_metric_gte.npy\", train_metric_embs)\n",
    "np.save(\"train_text_gte.npy\", train_text_embs)\n",
    "np.save(\"test_metric_gte.npy\", test_metric_embs)\n",
    "np.save(\"test_text_gte.npy\", test_text_embs)\n",
    "print(\"Saved GTE embeddings to disk.\")\n",
    "\n",
    "# -----------------------\n",
    "# Build features (concat, absdiff, prod, cos) same as earlier pipeline\n",
    "# -----------------------\n",
    "def build_features(metric_embs, text_embs):\n",
    "    assert metric_embs.shape == text_embs.shape\n",
    "    dot = np.sum(metric_embs * text_embs, axis=1)\n",
    "    norms = (np.linalg.norm(metric_embs, axis=1) * np.linalg.norm(text_embs, axis=1)) + 1e-9\n",
    "    cos = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "    absdiff = np.abs(metric_embs - text_embs).astype(np.float32)\n",
    "    prod = (metric_embs * text_embs).astype(np.float32)\n",
    "    concat = np.hstack([metric_embs.astype(np.float32), text_embs.astype(np.float32)])\n",
    "    X = np.hstack([concat, absdiff, prod, cos]).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "print(\"Building train features...\")\n",
    "X_gte = build_features(train_metric_embs, train_text_embs)\n",
    "print(\"Building test features...\")\n",
    "X_test_gte = build_features(test_metric_embs, test_text_embs)\n",
    "\n",
    "y = df[\"label\"].values.astype(np.float32)\n",
    "# if 'label' is a continuous score, round to nearest int for classification\n",
    "y_int = np.rint(y).astype(int)\n",
    "y_int = np.clip(y_int, 0, 10)\n",
    "\n",
    "print(\"Final shapes -> X:\", X_gte.shape, \"X_test:\", X_test_gte.shape, \"y:\", y_int.shape)\n",
    "np.save(\"X_gte.npy\", X_gte)\n",
    "np.save(\"X_test_gte.npy\", X_test_gte)\n",
    "np.save(\"y_int_gte.npy\", y_int)\n",
    "print(\"Saved feature matrices.\")\n",
    "\n",
    "# -----------------------\n",
    "# Train NLL-MLP classifier on new GTE features (reuse model structure from before)\n",
    "# -----------------------\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "class EmbClsDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class NLL_MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1=1024, hidden2=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# training params (tune)\n",
    "NFOLDS = 5\n",
    "EPOCHS = 12\n",
    "BATCH = 256\n",
    "LR = 1e-3\n",
    "WD = 1e-5\n",
    "PATIENCE = 3\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "oof_probs = np.zeros((len(X_gte), 11), dtype=np.float32)\n",
    "test_probs_folds = np.zeros((NFOLDS, X_test_gte.shape[0], 11), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_gte)):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    X_tr, X_val = X_gte[tr_idx], X_gte[val_idx]\n",
    "    y_tr, y_val = y_int[tr_idx], y_int[val_idx]\n",
    "\n",
    "    train_ds = EmbClsDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbClsDataset(X_val, y_val)\n",
    "    test_ds  = EmbClsDataset(X_test_gte)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model_cls = NLL_MLP(in_dim=X_gte.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model_cls.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_rmse = 1e9; best_state = None; patience = 0\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model_cls.train()\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model_cls(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_cls.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # validation\n",
    "        model_cls.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                logits = model_cls(xb)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                val_probs.append(probs)\n",
    "        val_probs = np.concatenate(val_probs, axis=0)\n",
    "        val_exp = (val_probs * np.arange(11)[None,:]).sum(axis=1)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y[val_idx], val_exp))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {ep}/{EPOCHS} | train_loss={running_loss/len(train_ds):.4f} | val_RMSE={val_rmse:.4f} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        if val_rmse < best_rmse - 1e-5:\n",
    "            best_rmse = val_rmse; best_state = model_cls.state_dict(); patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # load best\n",
    "    model_cls.load_state_dict(best_state)\n",
    "    model_cls.to(device)\n",
    "    model_cls.eval()\n",
    "\n",
    "    # OOF\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model_cls(xb)\n",
    "            val_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    val_probs = np.concatenate(val_probs, axis=0)\n",
    "    oof_probs[val_idx] = val_probs\n",
    "\n",
    "    # test probs\n",
    "    fold_test_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model_cls(xb)\n",
    "            fold_test_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    fold_test_probs = np.concatenate(fold_test_probs, axis=0)\n",
    "    test_probs_folds[fold] = fold_test_probs\n",
    "\n",
    "    print(f\"Fold {fold} best val RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Evaluate OOF\n",
    "oof_exp = (oof_probs * np.arange(11)[None,:]).sum(axis=1)\n",
    "oof_rmse = np.sqrt(mean_squared_error(y, oof_exp))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse)\n",
    "\n",
    "# Calibrate\n",
    "cal = LinearRegression().fit(oof_exp.reshape(-1,1), y)\n",
    "oof_cal = cal.predict(oof_exp.reshape(-1,1))\n",
    "print(\"OOF RMSE (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n",
    "print(\"Calibration params: a=\", cal.coef_[0], \"b=\", cal.intercept_)\n",
    "\n",
    "# Final test preds\n",
    "test_probs_mean = test_probs_folds.mean(axis=0)\n",
    "test_exp = (test_probs_mean * np.arange(11)[None,:]).sum(axis=1)\n",
    "test_exp_cal = cal.predict(test_exp.reshape(-1,1))\n",
    "test_exp_cal = np.clip(test_exp_cal, 0, 10)\n",
    "\n",
    "# Save artifacts + submission\n",
    "np.save(\"oof_probs_gte.npy\", oof_probs)\n",
    "np.save(\"test_probs_gte.npy\", test_probs_mean)\n",
    "np.save(\"oof_gte.npy\", oof_exp)\n",
    "np.save(\"test_gte.npy\", test_exp)\n",
    "\n",
    "test_df[\"ID\"] = np.arange(1, len(test_exp_cal)+1)\n",
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": test_exp_cal})\n",
    "sub.to_csv(\"submission_gte_nll_mlp.csv\", index=False)\n",
    "print(\"\\nSaved submission_gte_nll_mlp.csv\")\n",
    "print(\"OOF RMSE final (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nAO27ZgHQCo"
   },
   "source": [
    "GTE MULTILINGUAL WALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266,
     "referenced_widgets": [
      "31a8365bccbf496896082d325df4dded",
      "7478d4b3ead14db4bc635193d25c104c",
      "f5df57a9ce6048f89ab59ee3f68c5a16",
      "c40c68815bb049e99a9a0b068b03d720",
      "d5ecb25e14384c2a8103fe25cd623888",
      "047c86cf222b49b1b72b768064567904",
      "372d8a3af9054f959a809193a7f36a1c",
      "9f65c378a1bd452bb549bad4d3a04a74",
      "e47c36baa4504462a74ea2ed42b5bdb3",
      "47128008b33d429c8a1d5ccb95a5848d",
      "72c102b7667c492da389e64165fb83a5",
      "ab640226f8c64c7da0aeeafa70f9a753",
      "0a150c74124648758f1b77081c2a26fc",
      "060a69d6eb3c401bad65db8fc88ee64d",
      "9f0f225ea4e6494e9df7f1c30f041604",
      "a862d8ecd57a4998b3089f25ea4943d9",
      "7b813f3e27874b768bfa8603afca7c73",
      "a1397ca125ec4d4e8afe6ecf994c20a1",
      "b56e7d2d0fb242cdb42895d62bbe2fa2",
      "f9a4b6de87404306a728ab00180b8117",
      "bff32d1f0c1e4c709cc9f35c308c2b0f",
      "4a7ec4753f0648e7b5b47af8d51a3bc5",
      "f2f4d9f592ac4a86a3f2b6c2671b5c4a",
      "0a7308aab1e944e5adff8344b41c6d88",
      "8af50aee80924d3084185e676565c8ab",
      "c318a07773404a27b7e22f8aba178ded",
      "dab1dcd8a9a24f6599032a93ea68d5fd",
      "1fb1e923da294c1290935458fe76b594",
      "c44957c133e34493845da29b6f691e43",
      "b3881f7a71b24f3f99b39a0277abb394",
      "5e03458961544f8aaa325948aa7faf13",
      "e37c47da93bb457eaa54128cc8a7fd0f",
      "1c2b259023434f4c905d405525ea1580",
      "50996378765f4c168d7880aeab9b1834",
      "bfa0212a388f44669aca6bba944a5d3b",
      "8835cfc544dd4737a15a62453ada9986",
      "778885730a2f4f0fba8d90326fa7454a",
      "9ed5f1e5fdb24abb87f773db8d67171b",
      "ce50070e179844f6a990f31dfd4f11fa",
      "cfcfcd2d9f4b43fbb9015b4b42959bb1",
      "ba1339bab4064d89bce89f8d0935772a",
      "5b68bac7520f4966989c8ac1e3393ecc",
      "735cb9227fb9428482164e7e737cb6c9",
      "e021ad6c513141e8afa460f47438e94e",
      "3acc9d2cf2a74e199c713c6a4c88aa3c",
      "dfd0088881e04431b4aa7697bedb7def",
      "ebad798f0a44463f92ae5ed4db8b7e9b",
      "8f6fcefc4d4c4c24bc8e65c5c5301946",
      "bf1a360f641a4c08ae6fd19b12852fec",
      "4a29ed683e694c53a41017b2bad6f17f",
      "7815587f69ba4eb59019af3fab49cc0e",
      "79523e8ccd9d44c98732c9120bdaeebc",
      "bff4f0a66e5749459abb482d1a7b6c07",
      "7bc109d1923c4fe18fe9e3efced98061",
      "7b9de0e9e13e4461a4d09b33cc8ab71c"
     ]
    },
    "id": "OZ3YTxo-HPfX",
    "outputId": "87eee16f-1d12-461d-e732-4919b93ad2f9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-multilingual-base\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"Alibaba-NLP/gte-multilingual-base\",\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "print(\"Loaded multilingual GTE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee6xRrWn9-l-"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_gte_embedding(texts, batch_size=32):\n",
    "    all_embs = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tok(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            emb = out.last_hidden_state[:, 0, :]   # CLS pooling\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "        all_embs.append(emb.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8_XgXaJItgj"
   },
   "source": [
    "START NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxZi9dlO9-7Y",
    "outputId": "467b3434-b2cc-4863-e8ea-be1ff8f5c69b"
   },
   "outputs": [],
   "source": [
    "# Cell 1 — imports and device\n",
    "import os, time, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiOyvyTA9_gm",
    "outputId": "833203ac-8fef-470f-81a4-659574933c09"
   },
   "outputs": [],
   "source": [
    "# Cell 2 — load multilingual GTE (Alibaba)\n",
    "MODEL_NAME = \"Alibaba-NLP/gte-multilingual-base\"   # multilingual model (recommended)\n",
    "print(\"Loading tokenizer and model:\", MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# trust_remote_code may be required for some Alibaba models; include if HF asks for it\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "print(\"Loaded model ->\", MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "Q0MkuvrCI1un",
    "outputId": "2bfb86b1-1a6f-4e6b-e991-04eaaa828555"
   },
   "outputs": [],
   "source": [
    "# Cell 3 — load raw data and build combined_text / metric_text\n",
    "import json\n",
    "\n",
    "# Load train_data.json and test_data.json (assumes files in working dir)\n",
    "with open(\"train_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "    train_raw = json.load(f)\n",
    "with open(\"test_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "    test_raw = json.load(f)\n",
    "\n",
    "train_df = pd.DataFrame(train_raw)\n",
    "test_df  = pd.DataFrame(test_raw)\n",
    "\n",
    "def combine_all(row):\n",
    "    sp = str(row.get(\"system_prompt\",\"\")) if row.get(\"system_prompt\") else \"\"\n",
    "    up = str(row.get(\"user_prompt\",\"\")) if row.get(\"user_prompt\") else \"\"\n",
    "    rp = str(row.get(\"response\",\"\")) if row.get(\"response\") else \"\"\n",
    "    return sp + \" [SYS] \" + up + \" [USR] \" + rp + \" [RES]\"\n",
    "\n",
    "# Ensure combined_text and metric_text exist\n",
    "train_df[\"combined_text\"] = train_df.apply(combine_all, axis=1)\n",
    "train_df[\"metric_text\"]   = train_df[\"metric_name\"].astype(str)\n",
    "\n",
    "test_df[\"combined_text\"] = test_df.apply(combine_all, axis=1)\n",
    "test_df[\"metric_text\"]   = test_df[\"metric_name\"].astype(str)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsV_K0d2I1jO"
   },
   "outputs": [],
   "source": [
    "# Cell 4 — pooling + batched encoder helper\n",
    "MAX_LEN = 512\n",
    "ENC_BATCH = 32   # reduce to 16 or 8 if OOM\n",
    "\n",
    "def mean_pool(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "    masked = last_hidden_state * mask\n",
    "    summed = masked.sum(dim=1)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "    mean_pooled = summed / counts\n",
    "    return mean_pooled\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, desc=\"Encode\", batch_size=ENC_BATCH, max_len=MAX_LEN):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attn = enc[\"attention_mask\"].to(device)\n",
    "        out = model(input_ids=input_ids, attention_mask=attn, return_dict=True)\n",
    "        last = out.last_hidden_state  # (B, L, D)\n",
    "        pooled = mean_pool(last, attn)  # (B, D)\n",
    "        # optional L2 normalize\n",
    "        pooled = F.normalize(pooled, p=2, dim=1)\n",
    "        all_embs.append(pooled.cpu().numpy())\n",
    "    all_embs = np.vstack(all_embs)\n",
    "    return all_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249,
     "referenced_widgets": [
      "8aef23c5ed704cad866550e55428ff57",
      "474c2c21b9a547dea5c77206ed13c688",
      "2126f217c58a46b9b3370f24486349d1",
      "201a4861727e45a8921e5e38ccc2cb7d",
      "b1fdca69e75c4e52bd6822bf3fa71c64",
      "1675acfbfe2b48df8f9d036bb22d5413",
      "e90bfb318c434ed5a30597ecc9c8283c",
      "7f48b0dc91f343a899c35b3ea030b0c7",
      "1187e4a9c34b4681bd2157a26784cf18",
      "b498792606ef408981c144daadf34510",
      "7d7b75f7848249a4b4d77984878d4a00",
      "991d81b0de2745cd9623c6945501925d",
      "6cc483a5d7644114be943a13dd64f36d",
      "197c84b3f55647f683f1d0904139694e",
      "4da8ba5836ae4afc89059ddbbf6a116f",
      "67040c880b5b4dc4a1466aa35fd9ef29",
      "6f8b3024139f4a4fa5fe16662d93d11f",
      "16cecf79f3474ed1a6e50df932f18407",
      "b4917c6218eb45a29918efb27d09b5ce",
      "a62120e96de641e09445eb1a89398649",
      "3b598d2e4ed8417386825d51d24e3607",
      "aa85700a20bc47b785ed74db002dac9c",
      "78379cee348f4f5fbae7d0938c98aacd",
      "6267718daba64c3e82c6712fe09c8936",
      "7243639693a54280bcd11a0d8f459cb6",
      "6d201e064f48415abf1c92f7ec63b761",
      "e8326dc468664298bba9fdc48acea9ed",
      "66daa47b82a14757871d0f0dfe3e9cb6",
      "0a16dbffa76d44c49318f929a7e34d32",
      "9f58201cea0e4666b9876499838182e7",
      "8f8d6e7606174803ad7b6c8e0f37733e",
      "b3e74ba4372941e1b690036dda431f9b",
      "0d0081e082ad425a9443936cdbe354d3",
      "d67fcdaef95243ff8d6fcd5f50edd1a4",
      "23fd041fd4454a558c1baacc22520aef",
      "88ee17b026144088ad1b0a35a96793e3",
      "093f20a6ac0d4f4e933a8ff2cd983a95",
      "4a4f4f978318495e8c5cbffff2db5377",
      "17b62c45243e454e83a588e97ce89e00",
      "a051240f0c7a492a87792671dc78b40e",
      "4ec631d53f1d47dd8bf7e8801752b554",
      "da470c4277d342ca9dd43329ebed6932",
      "fa9e59c7d3524b39b3102b6a0e3b4a9d",
      "9becb9af4b1d4b2a82fea4f2d8d12cea"
     ]
    },
    "id": "MDrZvvVfI1a7",
    "outputId": "4007504e-5b95-466d-d6eb-45ad6e84e894"
   },
   "outputs": [],
   "source": [
    "# Cell 5 — encode train/test metric and combined texts\n",
    "train_metric_texts = train_df[\"metric_text\"].astype(str).tolist()\n",
    "train_combined_texts = train_df[\"combined_text\"].astype(str).tolist()\n",
    "test_metric_texts = test_df[\"metric_text\"].astype(str).tolist()\n",
    "test_combined_texts = test_df[\"combined_text\"].astype(str).tolist()\n",
    "\n",
    "print(\"Encoding train metric_texts...\")\n",
    "train_metric_embs = encode_texts(train_metric_texts, desc=\"train_metric\")\n",
    "print(\"Encoding train combined_texts...\")\n",
    "train_text_embs = encode_texts(train_combined_texts, desc=\"train_text\")\n",
    "print(\"Encoding test metric_texts...\")\n",
    "test_metric_embs = encode_texts(test_metric_texts, desc=\"test_metric\")\n",
    "print(\"Encoding test combined_texts...\")\n",
    "test_text_embs = encode_texts(test_combined_texts, desc=\"test_text\")\n",
    "\n",
    "print(\"Shapes:\", train_metric_embs.shape, train_text_embs.shape, test_metric_embs.shape, test_text_embs.shape)\n",
    "\n",
    "# Save embeddings to disk for reuse\n",
    "np.save(\"train_metric_gte_m.npy\", train_metric_embs)\n",
    "np.save(\"train_text_gte_m.npy\", train_text_embs)\n",
    "np.save(\"test_metric_gte_m.npy\", test_metric_embs)\n",
    "np.save(\"test_text_gte_m.npy\", test_text_embs)\n",
    "print(\"Saved GTE multilingual embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mz8UJCSjI1R2",
    "outputId": "84b20351-3660-4dcc-d669-efa06ebe9ab4"
   },
   "outputs": [],
   "source": [
    "# Cell 6 — negative generation (same logic as your earlier code)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "train_metric = np.load(\"train_metric_gte_m.npy\")\n",
    "train_text   = np.load(\"train_text_gte_m.npy\")\n",
    "y_real       = train_df[\"score\"].values.astype(np.float32)\n",
    "N = len(train_metric)\n",
    "print(\"N =\", N)\n",
    "\n",
    "# 1) Shuffle-based negatives (m same, text permuted)\n",
    "perm = rng.permutation(N)\n",
    "neg_metric_1 = train_metric\n",
    "neg_text_1   = train_text[perm]\n",
    "neg_y_1      = rng.integers(0, 3, size=N)\n",
    "\n",
    "# 2) Noise-corrupted negatives (text + gaussian noise)\n",
    "noise = rng.normal(scale=0.6, size=train_text.shape).astype(np.float32)\n",
    "neg_metric_2 = train_metric\n",
    "neg_text_2   = (train_text + noise).astype(np.float32)\n",
    "neg_y_2      = rng.integers(0, 3, size=N)\n",
    "\n",
    "# 3) Metric swap negatives\n",
    "perm2 = rng.permutation(N)\n",
    "neg_metric_3 = train_metric[perm2]\n",
    "neg_text_3   = train_text\n",
    "neg_y_3      = rng.integers(0, 3, size=N)\n",
    "\n",
    "# Combine everything\n",
    "m_all = np.vstack([train_metric, neg_metric_1, neg_metric_2, neg_metric_3]).astype(np.float32)\n",
    "t_all = np.vstack([train_text,   neg_text_1,   neg_text_2,   neg_text_3]).astype(np.float32)\n",
    "y_all = np.concatenate([y_real,  neg_y_1,      neg_y_2,      neg_y_3]).astype(np.float32)\n",
    "\n",
    "print(\"Combined shapes ->\", m_all.shape, t_all.shape, y_all.shape)\n",
    "np.save(\"m_all_gte.npy\", m_all)\n",
    "np.save(\"t_all_gte.npy\", t_all)\n",
    "np.save(\"y_all_gte.npy\", y_all)\n",
    "print(\"Saved m_all_gte.npy, t_all_gte.npy, y_all_gte.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q1H1U0CI1Di",
    "outputId": "707c1711-9f2e-46b4-d0ee-fbe743bcaca3"
   },
   "outputs": [],
   "source": [
    "# Cell 7 — build features exactly as before (concat, absdiff, prod, cos)\n",
    "m_all = np.load(\"m_all_gte.npy\")\n",
    "t_all = np.load(\"t_all_gte.npy\")\n",
    "y_all = np.load(\"y_all_gte.npy\")\n",
    "\n",
    "# cosine\n",
    "dot = np.sum(m_all * t_all, axis=1)\n",
    "norms = (np.linalg.norm(m_all, axis=1) * np.linalg.norm(t_all, axis=1)) + 1e-9\n",
    "cos = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "\n",
    "absdiff = np.abs(m_all - t_all).astype(np.float32)\n",
    "prod = (m_all * t_all).astype(np.float32)\n",
    "concat = np.hstack([m_all.astype(np.float32), t_all.astype(np.float32)])  # (N, 2*D)\n",
    "\n",
    "X_all = np.hstack([concat, absdiff, prod, cos]).astype(np.float32)\n",
    "print(\"X_all shape:\", X_all.shape, \"y_all shape:\", y_all.shape)\n",
    "\n",
    "# Build X_test from test embeddings\n",
    "test_metric = np.load(\"test_metric_gte_m.npy\")\n",
    "test_text   = np.load(\"test_text_gte_m.npy\")\n",
    "dot = np.sum(test_metric * test_text, axis=1)\n",
    "norms = (np.linalg.norm(test_metric, axis=1) * np.linalg.norm(test_text, axis=1)) + 1e-9\n",
    "cos_test = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "absdiff_test = np.abs(test_metric - test_text).astype(np.float32)\n",
    "prod_test = (test_metric * test_text).astype(np.float32)\n",
    "concat_test = np.hstack([test_metric.astype(np.float32), test_text.astype(np.float32)])\n",
    "X_test = np.hstack([concat_test, absdiff_test, prod_test, cos_test]).astype(np.float32)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Save\n",
    "np.save(\"X_all_gte.npy\", X_all)\n",
    "np.save(\"y_all_gte.npy\", y_all)\n",
    "np.save(\"X_test_gte.npy\", X_test)\n",
    "print(\"Saved X_all_gte.npy, y_all_gte.npy, X_test_gte.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeI0NVgzKgNz",
    "outputId": "55b826bd-66b7-42d0-d974-838ce3314b54"
   },
   "outputs": [],
   "source": [
    "# Cell 8 — train NLL MLP on X_all_gte.npy features (11-class classification)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# hyperparams (tune if needed)\n",
    "SEED = 42\n",
    "NFOLDS = 5\n",
    "EPOCHS = 12\n",
    "BATCH = 256\n",
    "LR = 1e-3\n",
    "WD = 1e-5\n",
    "PATIENCE = 3\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "X = np.load(\"X_all_gte.npy\").astype(np.float32)\n",
    "y = np.load(\"y_all_gte.npy\").astype(np.float32)\n",
    "X_test = np.load(\"X_test_gte.npy\").astype(np.float32)\n",
    "\n",
    "# ensure labels are integer 0..10\n",
    "y_int = np.rint(y).astype(int)\n",
    "y_int = np.clip(y_int, 0, 10)\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \"y:\", y_int.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "class EmbClsDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class NLL_MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1=1024, hidden2=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden2, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 11)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "oof_probs = np.zeros((len(X), 11), dtype=np.float32)\n",
    "test_probs_folds = np.zeros((NFOLDS, X_test.shape[0], 11), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y_int[tr_idx], y_int[val_idx]\n",
    "\n",
    "    train_ds = EmbClsDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbClsDataset(X_val, y_val)\n",
    "    test_ds  = EmbClsDataset(X_test)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model_cls = NLL_MLP(in_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model_cls.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_rmse = 1e9; best_state = None; patience = 0\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model_cls.train()\n",
    "        running_loss = 0.0\n",
    "        t0 = time.time()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model_cls(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model_cls.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # validation\n",
    "        model_cls.eval()\n",
    "        val_probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                logits = model_cls(xb)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                val_probs.append(probs)\n",
    "        val_probs = np.concatenate(val_probs, axis=0)\n",
    "        val_exp = (val_probs * np.arange(11)[None,:]).sum(axis=1)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y[val_idx], val_exp))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {ep}/{EPOCHS} | train_loss={running_loss/len(train_ds):.4f} | val_RMSE={val_rmse:.4f} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        if val_rmse < best_rmse - 1e-5:\n",
    "            best_rmse = val_rmse; best_state = model_cls.state_dict(); patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # load best state\n",
    "    model_cls.load_state_dict(best_state)\n",
    "    model_cls.to(device)\n",
    "    model_cls.eval()\n",
    "\n",
    "    # OOF probs\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model_cls(xb)\n",
    "            val_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    val_probs = np.concatenate(val_probs, axis=0)\n",
    "    oof_probs[val_idx] = val_probs\n",
    "\n",
    "    # fold test probs\n",
    "    fold_test_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model_cls(xb)\n",
    "            fold_test_probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    fold_test_probs = np.concatenate(fold_test_probs, axis=0)\n",
    "    test_probs_folds[fold] = fold_test_probs\n",
    "\n",
    "    print(f\"Fold {fold} best val RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# Evaluate OOF\n",
    "oof_exp = (oof_probs * np.arange(11)[None,:]).sum(axis=1)\n",
    "oof_rmse = np.sqrt(mean_squared_error(y, oof_exp))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse)\n",
    "\n",
    "# Calibrate and final test preds\n",
    "cal = LinearRegression().fit(oof_exp.reshape(-1,1), y)\n",
    "oof_cal = cal.predict(oof_exp.reshape(-1,1))\n",
    "print(\"OOF RMSE (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n",
    "print(\"Calibration params: a=\", cal.coef_[0], \"b=\", cal.intercept_)\n",
    "\n",
    "test_probs_mean = test_probs_folds.mean(axis=0)\n",
    "test_exp = (test_probs_mean * np.arange(11)[None,:]).sum(axis=1)\n",
    "test_exp_cal = cal.predict(test_exp.reshape(-1,1))\n",
    "test_exp_cal = np.clip(test_exp_cal, 0, 10)\n",
    "\n",
    "# Save outputs + submission\n",
    "np.save(\"oof_probs_gte_m.npy\", oof_probs)\n",
    "np.save(\"test_probs_gte_m.npy\", test_probs_mean)\n",
    "np.save(\"oof_gte_m.npy\", oof_exp)\n",
    "np.save(\"test_gte_m.npy\", test_exp)\n",
    "\n",
    "test_df[\"ID\"] = np.arange(1, len(test_exp_cal)+1)\n",
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": test_exp_cal})\n",
    "sub.to_csv(\"submission_gte_multilingual_nll_mlp.csv\", index=False)\n",
    "print(\"\\nSaved submission_gte_multilingual_nll_mlp.csv\")\n",
    "print(\"OOF RMSE final (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1gyXta7Kwmr"
   },
   "source": [
    "END HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Gq0WSmUKu9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1_5BFsq79aZ",
    "outputId": "63e7fca7-383f-4fc7-b27a-246c1b6ae329"
   },
   "outputs": [],
   "source": [
    "# STEP 5 — Train MLP with K-Fold CV, get OOF, calibrate, test inference\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load training features\n",
    "X = np.load(\"X_all.npy\").astype(np.float32)\n",
    "y = np.load(\"y_all.npy\").astype(np.float32)\n",
    "\n",
    "# Load test metric/text embeddings\n",
    "test_metric = np.load(\"test_metric_embs.npy\")\n",
    "test_text   = np.load(\"test_text_embs.npy\")\n",
    "\n",
    "# ---- build test features same way ----\n",
    "dot = np.sum(test_metric * test_text, axis=1)\n",
    "norms = (np.linalg.norm(test_metric, axis=1) * np.linalg.norm(test_text, axis=1)) + 1e-9\n",
    "cos_test = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "\n",
    "absdiff_test = np.abs(test_metric - test_text).astype(np.float32)\n",
    "prod_test    = (test_metric * test_text).astype(np.float32)\n",
    "concat_test  = np.hstack([test_metric.astype(np.float32),\n",
    "                          test_text.astype(np.float32)])\n",
    "\n",
    "X_test = np.hstack([concat_test, absdiff_test, prod_test, cos_test]).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset\n",
    "# -------------------------------\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# MLP Model definition\n",
    "# -------------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Training hyperparameters\n",
    "# -------------------------------\n",
    "EPOCHS = 20\n",
    "BATCH = 256\n",
    "LR = 1e-3\n",
    "NFOLDS = 5\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X), dtype=np.float32)\n",
    "test_preds = np.zeros((NFOLDS, len(X_test)), dtype=np.float32)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    train_ds = EmbDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbDataset(X_val, y_val)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    model = MLP(in_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_rmse = 999\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                p = model(xb).detach().cpu().numpy()\n",
    "                val_preds_list.append(p)\n",
    "\n",
    "        val_preds = np.concatenate(val_preds_list)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - TrainLoss: {running_loss/len(train_ds):.4f}  ValRMSE: {val_rmse:.4f}\")\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            torch.save(model.state_dict(), f\"mlp_fold{fold}.pt\")\n",
    "\n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(f\"mlp_fold{fold}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    # generate OOF preds\n",
    "    val_preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            p = model(xb).detach().cpu().numpy()\n",
    "            val_preds_list.append(p)\n",
    "\n",
    "    oof[val_idx] = np.concatenate(val_preds_list)\n",
    "\n",
    "    # test predictions\n",
    "    test_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), BATCH):\n",
    "            xb = torch.from_numpy(X_test[i:i+BATCH]).to(device)\n",
    "            p = model(xb).detach().cpu().numpy()\n",
    "            test_pred_list.append(p)\n",
    "\n",
    "    test_preds[fold] = np.concatenate(test_pred_list)\n",
    "\n",
    "    print(f\"Fold {fold} best RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Calibration (Label shift fix!)\n",
    "# -------------------------------\n",
    "oof_rmse = np.sqrt(mean_squared_error(y, oof))\n",
    "print(\"\\nOOF RMSE before calibration:\", oof_rmse)\n",
    "\n",
    "lr = LinearRegression().fit(oof.reshape(-1,1), y)\n",
    "a = float(lr.coef_[0])\n",
    "b = float(lr.intercept_)\n",
    "print(\"Calibration: y ≈ a*x + b =\", a, b)\n",
    "\n",
    "oof_cal = lr.predict(oof.reshape(-1,1))\n",
    "oof_cal_rmse = np.sqrt(mean_squared_error(y, oof_cal))\n",
    "print(\"OOF RMSE after calibration:\", oof_cal_rmse)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Final test predictions\n",
    "# -------------------------------\n",
    "test_pred_raw = test_preds.mean(axis=0)\n",
    "test_pred_cal = lr.predict(test_pred_raw.reshape(-1,1))\n",
    "test_pred_cal = np.clip(test_pred_cal, 0, 10).reshape(-1)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Save submission (Corrected)\n",
    "# -------------------------------\n",
    "\n",
    "# Ensure test_df exists (use the one already loaded earlier)\n",
    "# test_df is already in memory because we used it to create embeddings\n",
    "\n",
    "# Create ID column (1 to N) to match sample_submission.csv\n",
    "test_df[\"ID\"] = np.arange(1, len(test_df) + 1)\n",
    "\n",
    "# Build submission DataFrame\n",
    "sub = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"score\": test_pred_cal\n",
    "})\n",
    "\n",
    "# Save CSV\n",
    "sub.to_csv(\"submission_mlp_calibrated.csv\", index=False)\n",
    "print(\"\\nSaved submission_mlp_calibrated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeIn4Dt4NQ9x",
    "outputId": "0c85e235-8a35-40fb-b4aa-35e6552d542e"
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# STEP 5 — Train Strong MLP with K-Fold CV + Calibration\n",
    "# ======================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================================================\n",
    "# Load train features + labels\n",
    "# ======================================================\n",
    "X = np.load(\"X_all.npy\").astype(np.float32)\n",
    "y = np.load(\"y_all.npy\").astype(np.float32)\n",
    "\n",
    "# ======================================================\n",
    "# Load test embeddings + build test features\n",
    "# ======================================================\n",
    "test_metric = np.load(\"test_metric_embs.npy\")\n",
    "test_text   = np.load(\"test_text_embs.npy\")\n",
    "\n",
    "dot = np.sum(test_metric * test_text, axis=1)\n",
    "norms = (np.linalg.norm(test_metric, axis=1) * np.linalg.norm(test_text, axis=1)) + 1e-9\n",
    "cos_test = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "\n",
    "absdiff_test = np.abs(test_metric - test_text).astype(np.float32)\n",
    "prod_test    = (test_metric * test_text).astype(np.float32)\n",
    "concat_test  = np.hstack([test_metric.astype(np.float32), test_text.astype(np.float32)])\n",
    "\n",
    "X_test = np.hstack([concat_test, absdiff_test, prod_test, cos_test]).astype(np.float32)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Dataset Class\n",
    "# ======================================================\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Strong MLP (Residual + LayerNorm + GELU)\n",
    "# ======================================================\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, drop=0.15):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(dim, dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.lin2 = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.lin2(x)\n",
    "        return residual + x\n",
    "\n",
    "\n",
    "class StrongMLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        hidden = 512\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        # 6 deep residual blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResBlock(hidden),\n",
    "            ResBlock(hidden),\n",
    "            ResBlock(hidden),\n",
    "            ResBlock(hidden),\n",
    "            ResBlock(hidden),\n",
    "            ResBlock(hidden),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Training Hyperparameters\n",
    "# ======================================================\n",
    "EPOCHS = 30        # deeper model → more epochs\n",
    "BATCH = 256\n",
    "LR = 2e-4          # smaller LR → more stable\n",
    "NFOLDS = 5\n",
    "\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X), dtype=np.float32)\n",
    "test_preds = np.zeros((NFOLDS, len(X_test)), dtype=np.float32)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "# ======================================================\n",
    "# K-Fold Training Loop\n",
    "# ======================================================\n",
    "for train_idx, val_idx in kf.split(X):\n",
    "    print(f\"\\n================ Fold {fold} ================\")\n",
    "\n",
    "    X_tr, X_val = X[train_idx], X[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    train_ds = EmbDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbDataset(X_val, y_val)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "    model = StrongMLP(in_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_rmse = 999\n",
    "\n",
    "    # ------------------ Train Epochs ------------------\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        # ------------------ Validation ------------------\n",
    "        model.eval()\n",
    "        val_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                output = model(xb).detach().cpu().numpy()\n",
    "                val_list.append(output)\n",
    "\n",
    "        val_preds = np.concatenate(val_list)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}  \"\n",
    "              f\"TrainLoss={train_loss/len(train_ds):.4f}  \"\n",
    "              f\"ValRMSE={val_rmse:.4f}\")\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            torch.save(model.state_dict(), f\"strong_mlp_fold{fold}.pt\")\n",
    "\n",
    "    print(f\"Fold {fold} BEST RMSE = {best_rmse:.4f}\")\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f\"strong_mlp_fold{fold}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    # -------- OOF predictions --------\n",
    "    val_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            out = model(xb).detach().cpu().numpy()\n",
    "            val_list.append(out)\n",
    "\n",
    "    oof[val_idx] = np.concatenate(val_list)\n",
    "\n",
    "    # -------- Test predictions --------\n",
    "    test_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_test), BATCH):\n",
    "            xb = torch.from_numpy(X_test[i:i+BATCH]).to(device)\n",
    "            out = model(xb).detach().cpu().numpy()\n",
    "            test_list.append(out)\n",
    "\n",
    "    test_preds[fold] = np.concatenate(test_list)\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Calibration\n",
    "# ======================================================\n",
    "oof_rmse_raw = np.sqrt(mean_squared_error(y, oof))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse_raw)\n",
    "\n",
    "lr = LinearRegression().fit(oof.reshape(-1,1), y)\n",
    "a = float(lr.coef_[0]); b = float(lr.intercept_)\n",
    "print(\"Calibration params:  a=\", a, \" b=\", b)\n",
    "\n",
    "oof_cal = lr.predict(oof.reshape(-1,1))\n",
    "oof_rmse_cal = np.sqrt(mean_squared_error(y, oof_cal))\n",
    "print(\"OOF RMSE (calibrated):\", oof_rmse_cal)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Final Test Prediction\n",
    "# ======================================================\n",
    "test_raw = test_preds.mean(axis=0)\n",
    "test_cal = lr.predict(test_raw.reshape(-1,1))\n",
    "test_cal = np.clip(test_cal, 0, 10).reshape(-1)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# Save Submission\n",
    "# ======================================================\n",
    "test_df[\"ID\"] = np.arange(1, len(test_df)+1)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"score\": test_cal\n",
    "})\n",
    "\n",
    "sub.to_csv(\"submission_strongmlp_calibrated.csv\", index=False)\n",
    "print(\"\\nSaved submission_strongmlp_calibrated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNXv6iiNRDVK",
    "outputId": "160ae047-afd4-442f-ef83-f8fd964dbda7"
   },
   "outputs": [],
   "source": [
    "# Full hybrid training: CE + Pairwise RankNet + SWA + EMA (KFold)\n",
    "# Paste this whole cell and run (assumes X_all.npy, y_all.npy, test_metric_embs.npy, test_text_embs.npy exist)\n",
    "\n",
    "import os, time, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# -----------------------\n",
    "# Config / hyperparams\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "NFOLDS = 5\n",
    "EPOCHS = 18               # you can increase (SWA helps)\n",
    "BATCH = 256\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "PAIRWISE_WEIGHT = 0.5     # relative weight for pairwise loss vs CE\n",
    "SWA_START_FRAC = 0.7      # start SWA after this fraction of epochs\n",
    "EMA_DECAY = 0.999         # EMA decay for parameter averaging\n",
    "\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# -----------------------\n",
    "# Load data (features already computed)\n",
    "# -----------------------\n",
    "X = np.load(\"X_all.npy\").astype(np.float32)\n",
    "y = np.load(\"y_all.npy\").astype(np.float32)   # continuous labels 0..10\n",
    "# Convert to integer classes for CE target (round and clip)\n",
    "y_int = np.rint(y).astype(int)\n",
    "y_int = np.clip(y_int, 0, NUM_CLASSES-1)\n",
    "\n",
    "# Build X_test like you did before\n",
    "test_metric = np.load(\"test_metric_embs.npy\")\n",
    "test_text   = np.load(\"test_text_embs.npy\")\n",
    "\n",
    "dot = np.sum(test_metric * test_text, axis=1)\n",
    "norms = (np.linalg.norm(test_metric, axis=1) * np.linalg.norm(test_text, axis=1)) + 1e-9\n",
    "cos_test = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "absdiff_test = np.abs(test_metric - test_text).astype(np.float32)\n",
    "prod_test = (test_metric * test_text).astype(np.float32)\n",
    "concat_test = np.hstack([test_metric.astype(np.float32), test_text.astype(np.float32)])\n",
    "X_test = np.hstack([concat_test, absdiff_test, prod_test, cos_test]).astype(np.float32)\n",
    "\n",
    "print(\"Shapes -> X:\", X.shape, \"y:\", y_int.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, self.y[idx]\n",
    "\n",
    "# -----------------------\n",
    "# Model: shallow expressive MLP\n",
    "# -----------------------\n",
    "class ShallowMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=512, dropout=0.12):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden//2, NUM_CLASSES)  # logits for 11 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# -----------------------\n",
    "# Pairwise RankNet loss helper\n",
    "#   Uses expected score (E[y] from softmax) as scalar \"s\".\n",
    "#   For pairs (i,j) create label t = 1 if y_i > y_j else 0.\n",
    "#   Loss = BCEWithLogitsLoss(s_i - s_j, t)\n",
    "# -----------------------\n",
    "bce_logits = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "def pairwise_rank_loss_from_logits(logits, targets, max_pairs=1024):\n",
    "    \"\"\"\n",
    "    logits: tensor (B, C)\n",
    "    targets: tensor (B,) integer classes\n",
    "    returns: mean pairwise logistic loss\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # expected scalar scores (E[class])\n",
    "        probs = torch.softmax(logits.detach(), dim=1)  # detach to not backprop through expectation if you want; but we use logits for pair diff so ok\n",
    "        classes = torch.arange(0, logits.shape[1], dtype=torch.float32, device=logits.device)\n",
    "        exp_scores = (probs * classes[None,:]).sum(dim=1)  # (B,)\n",
    "\n",
    "    B = logits.shape[0]\n",
    "    if B < 2:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    # sample up to max_pairs random pairs for scalability\n",
    "    max_pairs = min(max_pairs, B*(B-1)//2)\n",
    "    # Create random pair indices\n",
    "    idx = torch.randperm(B, device=logits.device)\n",
    "    # We'll do random pairing by choosing two shuffled copies and filtering equal targets\n",
    "    i_idx = idx\n",
    "    j_idx = torch.randperm(B, device=logits.device)\n",
    "    s_i = exp_scores[i_idx]\n",
    "    s_j = exp_scores[j_idx]\n",
    "    y_i = targets[i_idx]\n",
    "    y_j = targets[j_idx]\n",
    "    # Keep only pairs where y_i != y_j\n",
    "    mask = (y_i != y_j)\n",
    "    if mask.sum() == 0:\n",
    "        return torch.tensor(0.0, device=logits.device)\n",
    "    s_diff = s_i[mask] - s_j[mask]          # predict probability that i>j using s_diff\n",
    "    t = (y_i[mask] > y_j[mask]).float()     # target 1 if i>j else 0\n",
    "    if s_diff.numel() > max_pairs:\n",
    "        perm2 = torch.randperm(s_diff.numel(), device=logits.device)[:max_pairs]\n",
    "        s_diff = s_diff[perm2]\n",
    "        t = t[perm2]\n",
    "    loss = bce_logits(s_diff, t)\n",
    "    return loss\n",
    "\n",
    "# -----------------------\n",
    "# KFold loop with SWA + EMA\n",
    "# -----------------------\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_probs = np.zeros((len(X), NUM_CLASSES), dtype=np.float32)\n",
    "test_probs_folds = np.zeros((NFOLDS, X_test.shape[0], NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y_int[tr_idx], y_int[val_idx]\n",
    "\n",
    "    train_ds = EmbDataset(X_tr, y_tr)\n",
    "    val_ds = EmbDataset(X_val, y_val)\n",
    "    test_ds = EmbDataset(X_test)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, pin_memory=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = ShallowMLP(in_dim=X.shape[1]).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # SWA setup\n",
    "    swa_start = int(EPOCHS * SWA_START_FRAC)\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_scheduler = None  # set below if needed\n",
    "\n",
    "    # EMA shadow weights\n",
    "    ema_shadow = {name: param.detach().cpu().clone() for name, param in model.named_parameters()}\n",
    "    ema_n = 0\n",
    "\n",
    "    # criterion CE\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_rmse = 1e9\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    EARLY_STOP = 4\n",
    "\n",
    "    # LR scheduler: cosine annealing\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_ce = 0.0\n",
    "        total_pair = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)                       # (B, C)\n",
    "            loss_ce = ce_loss(logits, yb)\n",
    "            loss_pair = pairwise_rank_loss_from_logits(logits, yb, max_pairs=512)\n",
    "            loss = loss_ce + PAIRWISE_WEIGHT * loss_pair\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update EMA shadow\n",
    "            ema_n += 1\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        s = ema_shadow[name]\n",
    "                        # move current param to CPU tensor for numerical stability\n",
    "                        cur = param.detach().cpu()\n",
    "                        ema_shadow[name] = EMA_DECAY * s + (1.0 - EMA_DECAY) * cur\n",
    "\n",
    "            total_loss += float(loss.item()) * xb.size(0)\n",
    "            total_ce += float(loss_ce.item()) * xb.size(0)\n",
    "            total_pair += float(loss_pair.item()) * xb.size(0)\n",
    "            n_samples += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        # optionally update SWA\n",
    "        if ep > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "\n",
    "        # validation - use current model for validation\n",
    "        model.eval()\n",
    "        val_probs_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                val_probs_list.append(probs)\n",
    "        val_probs = np.concatenate(val_probs_list, axis=0)\n",
    "        val_exp = (val_probs * np.arange(NUM_CLASSES)[None,:]).sum(axis=1)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y[val_idx], val_exp))\n",
    "\n",
    "        avg_loss = total_loss / n_samples\n",
    "        print(f\"Epoch {ep}/{EPOCHS} | loss={avg_loss:.4f} (CE={total_ce/n_samples:.4f} Pair={total_pair/n_samples:.4f}) | val_RMSE={val_rmse:.4f} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # early save best model by val_rmse\n",
    "        if val_rmse + 1e-6 < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= EARLY_STOP and ep > 6:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Finished epochs for this fold\n",
    "    print(f\">>> Fold {fold} training done. Best val RMSE: {best_val_rmse:.4f}\")\n",
    "\n",
    "    # finalize SWA model: update BN statistics (if any) using train_dl\n",
    "    if isinstance(swa_model, AveragedModel):\n",
    "        # copy averaged weights to a temp model for evaluation\n",
    "        swa_state = swa_model.module.state_dict() if hasattr(swa_model, \"module\") else swa_model.state_dict()\n",
    "        # create eval model and load swa_state\n",
    "        eval_model = ShallowMLP(in_dim=X.shape[1]).to(device)\n",
    "        eval_model.load_state_dict(swa_state)\n",
    "        # update bn (if present) - here we have LayerNorm but ok:\n",
    "        # torch.optim.swa_utils.update_bn(train_dl, eval_model, device=device)  # optional\n",
    "    else:\n",
    "        eval_model = ShallowMLP(in_dim=X.shape[1]).to(device)\n",
    "        eval_model.load_state_dict(best_state)\n",
    "\n",
    "    # ALSO prepare EMA-evaluated model by copying shadow into a model\n",
    "    ema_model = ShallowMLP(in_dim=X.shape[1]).to(device)\n",
    "    # copy EMA params (shadow is CPU tensors)\n",
    "    ema_state = ema_model.state_dict()\n",
    "    for name in ema_state.keys():\n",
    "        if name in ema_shadow:\n",
    "            ema_state[name] = ema_shadow[name].to(device)\n",
    "    ema_model.load_state_dict(ema_state)\n",
    "\n",
    "    # Choose which to use for OOF prediction: you can try both\n",
    "    # We'll use SWA-eval (eval_model) for OOF and later ensemble fold predictions across folds\n",
    "    eval_model.eval()\n",
    "    val_probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = eval_model(xb)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            val_probs_list.append(probs)\n",
    "    val_probs = np.concatenate(val_probs_list, axis=0)\n",
    "    oof_probs[val_idx] = val_probs\n",
    "\n",
    "    # Test prediction for this fold using eval_model\n",
    "    test_fold_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb in DataLoader(test_ds, batch_size=BATCH, shuffle=False):\n",
    "            xb = torch.tensor(xb, dtype=torch.float32).to(device)\n",
    "            logits = eval_model(xb)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            test_fold_list.append(probs)\n",
    "    test_fold_preds = np.concatenate(test_fold_list, axis=0)\n",
    "    test_probs_folds[fold] = test_fold_preds\n",
    "\n",
    "    # Save fold model for safety\n",
    "    torch.save(eval_model.state_dict(), f\"ranknet_swa_fold{fold}.pt\")\n",
    "    print(f\"Saved ranknet_swa_fold{fold}.pt\")\n",
    "\n",
    "# -----------------------\n",
    "# OOF eval and calibration\n",
    "# -----------------------\n",
    "oof_exp = (oof_probs * np.arange(NUM_CLASSES)[None,:]).sum(axis=1)\n",
    "oof_rmse_raw = np.sqrt(mean_squared_error(y, oof_exp))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse_raw)\n",
    "\n",
    "# Calibrate with linear regression\n",
    "cal = LinearRegression().fit(oof_exp.reshape(-1,1), y)\n",
    "oof_cal = cal.predict(oof_exp.reshape(-1,1))\n",
    "print(\"OOF RMSE (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n",
    "print(\"Calibration params: a=\", cal.coef_[0], \"b=\", cal.intercept_)\n",
    "\n",
    "# -----------------------\n",
    "# Final test predictions\n",
    "# -----------------------\n",
    "test_probs_mean = test_probs_folds.mean(axis=0)\n",
    "test_exp = (test_probs_mean * np.arange(NUM_CLASSES)[None,:]).sum(axis=1)\n",
    "test_exp_cal = cal.predict(test_exp.reshape(-1,1))\n",
    "test_exp_cal = np.clip(test_exp_cal, 0, 10).reshape(-1)\n",
    "\n",
    "# Save submission\n",
    "# Make sure test_df exists in memory (we used it earlier) or reload sample submission index\n",
    "try:\n",
    "    test_df\n",
    "except NameError:\n",
    "    # if test_df not in memory, try reading test_data.json to get length\n",
    "    import json\n",
    "    with open(\"test_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "        test_raw = json.load(f)\n",
    "    test_df = pd.DataFrame(test_raw)\n",
    "\n",
    "test_df[\"ID\"] = np.arange(1, len(test_exp_cal)+1)\n",
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": test_exp_cal})\n",
    "sub.to_csv(\"submission_ranknet_swa_ema.csv\", index=False)\n",
    "print(\"\\nSaved submission_ranknet_swa_ema.csv\")\n",
    "print(\"OOF RMSE final (calibrated):\", np.sqrt(mean_squared_error(y, oof_cal)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pwTKYwfUgLC",
    "outputId": "4bcb4236-a940-4bad-f38c-2a3d0a9da560"
   },
   "outputs": [],
   "source": [
    "# FULL: Train MSE + KL(hist) with SWA+EMA, quantile map postprocess, save submission\n",
    "# Paste and run in the notebook where your embeddings/features exist.\n",
    "\n",
    "import os, time, math, random\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "NFOLDS = 5\n",
    "EPOCHS = 22           # increase if you have time\n",
    "BATCH = 256\n",
    "LR = 2.5e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# histogram loss params\n",
    "NUM_BINS = 200        # histogram resolution\n",
    "SIGMA_BIN = 0.15      # RBF width for soft histogram\n",
    "LAMBDA_KL = 0.06      # weight of KL term vs MSE (tune!)\n",
    "\n",
    "SWA_START_FRAC = 0.7  # start SWA after this fraction of epochs\n",
    "EMA_DECAY = 0.999\n",
    "\n",
    "# ----------------- Load features -----------------\n",
    "X = np.load(\"X_all.npy\").astype(np.float32)   # (N, D)\n",
    "y = np.load(\"y_all.npy\").astype(np.float32)   # continuous 0..10\n",
    "\n",
    "# build test features (same as you used before)\n",
    "test_metric = np.load(\"test_metric_embs.npy\")\n",
    "test_text   = np.load(\"test_text_embs.npy\")\n",
    "\n",
    "dot = np.sum(test_metric * test_text, axis=1)\n",
    "norms = (np.linalg.norm(test_metric, axis=1) * np.linalg.norm(test_text, axis=1)) + 1e-9\n",
    "cos_test = (dot / norms).reshape(-1,1).astype(np.float32)\n",
    "\n",
    "absdiff_test = np.abs(test_metric - test_text).astype(np.float32)\n",
    "prod_test    = (test_metric * test_text).astype(np.float32)\n",
    "concat_test  = np.hstack([test_metric.astype(np.float32), test_text.astype(np.float32)])\n",
    "\n",
    "X_test = np.hstack([concat_test, absdiff_test, prod_test, cos_test]).astype(np.float32)\n",
    "\n",
    "print(\"X\", X.shape, \"y\", y.shape, \"X_test\", X_test.shape)\n",
    "\n",
    "# ----------------- Target histogram (friend-like multimodal) -----------------\n",
    "# If you have an explicit target PDF from your friend, replace the construction below.\n",
    "def build_target_pdf(num_bins=NUM_BINS):\n",
    "    bins = np.linspace(0,10,num_bins+1)\n",
    "    centers = 0.5*(bins[:-1] + bins[1:])\n",
    "    # Two Gaussians + small mid mass - tune amplitudes/widths as per your friend's plot\n",
    "    pdf_low = np.exp(-0.5*((centers-0.9)/0.25)**2)\n",
    "    pdf_high = 1.0 * np.exp(-0.5*((centers-8.6)/0.6)**2)\n",
    "    pdf_mid = 0.08 * np.exp(-0.5*((centers-4.0)/1.3)**2)\n",
    "    pdf = pdf_low*1.0 + pdf_high + pdf_mid\n",
    "    pdf = np.maximum(pdf, 1e-12)\n",
    "    pdf = pdf / pdf.sum()\n",
    "    return centers.astype(np.float32), pdf.astype(np.float32)\n",
    "\n",
    "BIN_CENTERS, TARGET_PDF = build_target_pdf(NUM_BINS)\n",
    "TARGET_PDF_T = torch.tensor(TARGET_PDF, dtype=torch.float32, device=DEVICE)\n",
    "BIN_CENTERS_T = torch.tensor(BIN_CENTERS, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# ----------------- Helpers: soft histogram + KL loss -----------------\n",
    "def soft_histogram_torch(preds, bin_centers_t, sigma=SIGMA_BIN):\n",
    "    # preds: (B,), bin_centers_t: (M,)\n",
    "    # Output: normalized histogram (M,)\n",
    "    d = preds.unsqueeze(1) - bin_centers_t.unsqueeze(0)   # (B, M)\n",
    "    w = torch.exp(-0.5 * (d / sigma)**2)                 # (B, M)\n",
    "    hist = w.sum(dim=0)                                   # (M,)\n",
    "    hist = hist / (hist.sum() + 1e-12)\n",
    "    return hist\n",
    "\n",
    "def kl_hist_loss_torch(preds, target_pdf_t=TARGET_PDF_T, bin_centers_t=BIN_CENTERS_T, sigma=SIGMA_BIN):\n",
    "    hist = soft_histogram_torch(preds, bin_centers_t, sigma)\n",
    "    # KL(target || pred) or KL(pred||target)? we use KL(pred || target) via log(hist) - log(target) * hist\n",
    "    # use F.kl_div(logP, Q) expects logP input and target Q.\n",
    "    loss = F.kl_div((hist+1e-12).log(), target_pdf_t, reduction='batchmean')\n",
    "    return loss\n",
    "\n",
    "# ----------------- Dataset -----------------\n",
    "class EmbDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, self.y[idx]\n",
    "\n",
    "# ----------------- Shallow MLP model (robust) -----------------\n",
    "class RobustMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1=768, hidden2=256, drop=0.12):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden1),\n",
    "            nn.LayerNorm(hidden1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.LayerNorm(hidden2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(hidden2, 1)  # regression output\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "# ----------------- Training hyperparams -----------------\n",
    "kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n",
    "oof = np.zeros(len(X), dtype=np.float32)\n",
    "test_preds_folds = np.zeros((NFOLDS, X_test.shape[0]), dtype=np.float32)\n",
    "\n",
    "fold = 0\n",
    "for tr_idx, val_idx in kf.split(X):\n",
    "    print(\"\\n========== Fold\", fold, \"==========\")\n",
    "    X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    train_ds = EmbDataset(X_tr, y_tr)\n",
    "    val_ds   = EmbDataset(X_val, y_val)\n",
    "    test_ds  = EmbDataset(X_test)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = RobustMLP(in_dim=X.shape[1]).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "\n",
    "    # SWA & EMA setup\n",
    "    swa_start = int(EPOCHS * SWA_START_FRAC)\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_count = 0\n",
    "\n",
    "    # EMA shadow (on CPU for stable accumulation)\n",
    "    ema_shadow = {name: param.detach().cpu().clone() for name, param in model.named_parameters()}\n",
    "    ema_n = 0\n",
    "\n",
    "    best_val_rmse = 1e9\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "    EARLY_STOP = 4\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_kl = 0.0\n",
    "        ns = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            preds = model(xb)                 # (B,)\n",
    "            loss_mse = mse_loss(preds, yb)\n",
    "            loss_kl = kl_hist_loss_torch(preds, TARGET_PDF_T, BIN_CENTERS_T, SIGMA_BIN)\n",
    "            loss = loss_mse + LAMBDA_KL * loss_kl\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            # update EMA shadow (CPU)\n",
    "            ema_n += 1\n",
    "            with torch.no_grad():\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        s = ema_shadow[name]\n",
    "                        cur = param.detach().cpu()\n",
    "                        ema_shadow[name] = EMA_DECAY * s + (1.0 - EMA_DECAY) * cur\n",
    "\n",
    "            total_loss += float(loss.item()) * xb.size(0)\n",
    "            total_mse += float(loss_mse.item()) * xb.size(0)\n",
    "            total_kl += float(loss_kl.item()) * xb.size(0)\n",
    "            ns += xb.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # update SWA\n",
    "        if ep > swa_start:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_count += 1\n",
    "\n",
    "        # validation using current (non-SWA) model\n",
    "        model.eval()\n",
    "        preds_val_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb = xb.to(DEVICE)\n",
    "                p = model(xb).detach().cpu().numpy()\n",
    "                preds_val_list.append(p)\n",
    "        preds_val = np.concatenate(preds_val_list, axis=0)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "\n",
    "        avg_loss = total_loss / ns\n",
    "        print(f\"Epoch {ep}/{EPOCHS} | loss={avg_loss:.5f} (mse={total_mse/ns:.5f}, kl={total_kl/ns:.5f}) | val_RMSE={val_rmse:.4f} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        # early save best non-SWA model by val_rmse\n",
    "        if val_rmse + 1e-8 < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= EARLY_STOP and ep > 6:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print(f\"Fold {fold} finished. best_val_rmse={best_val_rmse:.4f} swa_count={swa_count}\")\n",
    "\n",
    "    # choose final eval model: SWA if available else best_state\n",
    "    if swa_count > 0:\n",
    "        eval_model = RobustMLP(in_dim=X.shape[1]).to(DEVICE)\n",
    "        swa_state = swa_model.module.state_dict() if hasattr(swa_model, \"module\") else swa_model.state_dict()\n",
    "        eval_model.load_state_dict(swa_state)\n",
    "    else:\n",
    "        eval_model = RobustMLP(in_dim=X.shape[1]).to(DEVICE)\n",
    "        eval_model.load_state_dict(best_state)\n",
    "\n",
    "    # Also build EMA model by copying shadow params\n",
    "    ema_model = RobustMLP(in_dim=X.shape[1]).to(DEVICE)\n",
    "    ema_state = ema_model.state_dict()\n",
    "    for n in ema_state.keys():\n",
    "        if n in ema_shadow:\n",
    "            ema_state[n] = ema_shadow[n].to(DEVICE)\n",
    "    ema_model.load_state_dict(ema_state)\n",
    "\n",
    "    # Use eval_model for OOF predictions (you can try ema_model instead)\n",
    "    eval_model.eval()\n",
    "    val_preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = eval_model(xb).detach().cpu().numpy()\n",
    "            val_preds_list.append(p)\n",
    "    val_preds = np.concatenate(val_preds_list, axis=0)\n",
    "    oof[val_idx] = val_preds\n",
    "\n",
    "    # test preds by fold\n",
    "    test_fold_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_dl:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = eval_model(xb).detach().cpu().numpy()\n",
    "            test_fold_list.append(p)\n",
    "    test_fold_preds = np.concatenate(test_fold_list, axis=0)\n",
    "    test_preds_folds[fold] = test_fold_preds\n",
    "\n",
    "    # save fold models\n",
    "    torch.save(eval_model.state_dict(), f\"robustmlp_fold{fold}.pt\")\n",
    "    torch.save(ema_model.state_dict(), f\"robustmlp_ema_fold{fold}.pt\")\n",
    "    print(f\"Saved robustmlp_fold{fold}.pt and ema version.\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# ----------------- OOF eval + calibration -----------------\n",
    "oof_rmse_raw = np.sqrt(mean_squared_error(y, oof))\n",
    "print(\"\\nOOF RMSE (raw):\", oof_rmse_raw)\n",
    "\n",
    "# Linear calibration\n",
    "lr_cal = LinearRegression().fit(oof.reshape(-1,1), y)\n",
    "oof_cal = lr_cal.predict(oof.reshape(-1,1))\n",
    "oof_rmse_cal = np.sqrt(mean_squared_error(y, oof_cal))\n",
    "print(\"OOF RMSE (calibrated):\", oof_rmse_cal)\n",
    "print(\"Calibration params: a=\", lr_cal.coef_[0], \"b=\", lr_cal.intercept_)\n",
    "\n",
    "# ----------------- Quantile / histogram post-processing (map test preds to target) -----------------\n",
    "# Build a sampled array from TARGET_PDF to draw target quantiles\n",
    "n_sample_map = 100000\n",
    "sampled_vals = np.random.choice(BIN_CENTERS, size=n_sample_map, p=TARGET_PDF)\n",
    "\n",
    "def quantile_match_array(preds_raw, sampled_vals):\n",
    "    # preds_raw: numpy (N,)\n",
    "    # returns mapped preds (N,) where quantiles of preds mapped to quantiles of sampled_vals\n",
    "    order = np.argsort(preds_raw)\n",
    "    ranks = np.empty_like(order)\n",
    "    ranks[order] = np.arange(len(preds_raw))\n",
    "    q = ranks.astype(np.float32) / (len(preds_raw)-1 + 1e-12)\n",
    "    mapped = np.quantile(sampled_vals, q)\n",
    "    return mapped\n",
    "\n",
    "# Build final test prediction: average fold predictions -> calibrate -> quantile map -> clip\n",
    "test_raw_mean = test_preds_folds.mean(axis=0)\n",
    "test_cal = lr_cal.predict(test_raw_mean.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "# Apply quantile mapping (fold-level mapping can be done but here use global)\n",
    "test_quant_mapped = quantile_match_array(test_cal, sampled_vals)\n",
    "test_final = np.clip(test_quant_mapped, 0.0, 10.0)\n",
    "\n",
    "# ----------------- Save submission -----------------\n",
    "# load test_df if not in memory\n",
    "try:\n",
    "    test_df\n",
    "except NameError:\n",
    "    import json\n",
    "    with open(\"test_data.json\",\"r\",encoding=\"utf8\") as f:\n",
    "        test_raw = json.load(f)\n",
    "    test_df = pd.DataFrame(test_raw)\n",
    "\n",
    "test_df[\"ID\"] = np.arange(1, len(test_final)+1)\n",
    "sub = pd.DataFrame({\"ID\": test_df[\"ID\"], \"score\": test_final})\n",
    "sub.to_csv(\"submission_histKL_swa_ema_quantmap.csv\", index=False)\n",
    "print(\"\\nSaved submission_histKL_swa_ema_quantmap.csv\")\n",
    "\n",
    "print(\"\\nFinal stats:\")\n",
    "print(\"OOF RMSE raw:\", oof_rmse_raw)\n",
    "print(\"OOF RMSE calibrated:\", oof_rmse_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l33wJH4XRM3"
   },
   "source": [
    "END ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-chaLZalTSz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
